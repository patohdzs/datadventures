{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakes and Quakes\n",
    "### *Ordinal Modelling of Earthquake-Induced Building Damages*\n",
    "### Patricio Hernandez Senosiain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn - preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, RobustScaler, FunctionTransformer, LabelEncoder\n",
    "\n",
    "# Scikit learn - model selection\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, KFold, cross_val_score, cross_validate\n",
    "\n",
    "# Scikit learn - model evaluation\n",
    "from sklearn.metrics import f1_score, make_scorer, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Scikit learn - modelling\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsOneClassifier, OutputCodeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# CatBoost - modelling\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# XGBoost - modelling\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# LightGBM - modelling\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Category Encoders - feature encoding\n",
    "from category_encoders import TargetEncoder, LeaveOneOutEncoder, CatBoostEncoder\n",
    "from category_encoders.wrapper import PolynomialWrapper\n",
    "\n",
    "# Pandas and Numpy - data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotly, Matplotlib, and Seaborn - visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from qbstyles import mpl_style\n",
    "\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib theme\n",
    "#mpl_style(dark=True)\n",
    "\n",
    "# Declaring standard Seaborn color palette\n",
    "standard_palette = []\n",
    "for i in range(10):\n",
    "    standard_palette += list(sns.color_palette('muted'))\n",
    "\n",
    "\n",
    "# Convergence warning disabling\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter\n",
    "    #simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "# Setting a random seed\n",
    "SEED = 105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('data/train_values.csv')\n",
    "test_features = pd.read_csv('data/test_values.csv') \n",
    "train_target = pd.read_csv('data/train_labels.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Features data:')\n",
    "train_features.info(verbose=False)\n",
    "print('')\n",
    "print('Test Features data:')\n",
    "test_features.info(verbose=False)\n",
    "print('Train Target data:')\n",
    "train_target.info(verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining train target to features data\n",
    "data = train_features.merge(train_target, on='building_id' )\n",
    "data = data.drop(columns='building_id')\n",
    "\n",
    "# Separating 'Id' column for test observations\n",
    "ID = test_features['building_id']\n",
    "\n",
    "# Joining test and training datasets\n",
    "data = data.append(test_features, sort=False)\n",
    "data = data.drop(columns='building_id') \n",
    "\n",
    "# Printing overview\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying features\n",
    "loc_feats = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id' ]\n",
    "cont_feats = ['area_percentage', 'height_percentage']\n",
    "count_feats = ['count_floors_pre_eq', 'age', 'count_families']\n",
    "cat_feats = ['land_surface_condition', 'foundation_type', 'roof_type', \n",
    "             'ground_floor_type', 'other_floor_type', 'position', \n",
    "             'plan_configuration', 'legal_ownership_status']\n",
    "\n",
    "binary_feats = data.drop(columns=cont_feats+count_feats+cat_feats+loc_feats).columns.tolist()\n",
    "binary_feats.remove('damage_grade') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['area_percentage'] = np.log1p(data['area_percentage']) \n",
    "\n",
    "data['age'] = np.log1p(data['age']/5)\n",
    "\n",
    "\n",
    "level_1 = range(31)\n",
    "level_2 = range(1428)\n",
    "level_3 = range(12568)\n",
    "\n",
    "loc_cats = [level_1, level_2, level_3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Although this is an ordinal regression problem, the metric used by Driven Data to evaluate models for this competition is the micro averaged F1 score across the three classes:\n",
    "\n",
    "$$ F_{micro} \\; = \\; \\frac{2 \\cdot P_{micro} \\cdot R_{micro} }{P_{micro} + R_{micro} }$$\n",
    "\n",
    "where $P_{micro}$ and $R_{micro}$ stand for the precision and recall metrics :\n",
    "\n",
    "\n",
    "$$ P_{micro} \\;=\\; \\frac{\\sum^3_{k=1} TP_k}{\\sum^3_{k=1}TP_k + FP_k} \\;,\\; R_{micro} \\;=\\; \\frac{\\sum^3_{k=1}TP_k}{\\sum^3_{k=1}TP_k + TN_k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(y_true, y_pred):\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    ConfusionMatrixDisplay(cm).plot()\n",
    "    print('Confusion Matrix')\n",
    "    print(cm)\n",
    "    print('')\n",
    "    \n",
    "    # Classification report\n",
    "    print('Classification Report')\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print('')\n",
    "    print('Micro F1 Score')\n",
    "    print(f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    return f1_score(y_true, y_pred, average='micro')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Feature Matrices and Target Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating training and test matrices\n",
    "train = data[data['damage_grade'].isnull()==False]\n",
    "test = data[data['damage_grade'].isnull()]\n",
    "\n",
    "# Creating feature matrix and target vector from training sample\n",
    "X_train = train.drop(columns='damage_grade')\n",
    "y_train = train['damage_grade']\n",
    "\n",
    "# Creating feature matrix from test sample\n",
    "X_test = test.drop(columns='damage_grade')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Location Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loc = X_train[loc_feats].copy() \n",
    "X_loc_test = X_test[loc_feats].copy() \n",
    "\n",
    "y_loc = pd.get_dummies(y_train, prefix='y', drop_first=True) \n",
    "\n",
    "X_loc = X_loc.join(y_loc)\n",
    "X_loc['y'] = y_train\n",
    "\n",
    "w=100\n",
    "for c in [2.0, 3.0]:\n",
    "    for i in range(1,4):\n",
    "        level = f'geo_level_{i}_id'\n",
    "        prior = f'level_{i-1}_y_{round(c)}'\n",
    "        posterior =  f'level_{i}_y_{round(c)}'\n",
    "        target = f'y_{c}'\n",
    "\n",
    "        if i==1:\n",
    "            prior_mean = X_loc[target].mean()\n",
    "        else:\n",
    "            prior_mean = X_loc.groupby(level)[prior].mean() \n",
    "\n",
    "        # Compute the number of values and the mean of each group\n",
    "        agg = X_loc.groupby(level)[target].agg(['count', 'mean'])\n",
    "        count = agg['count']\n",
    "        mean = agg['mean']\n",
    "\n",
    "        # Compute the Laplace smoothened means\n",
    "        smooth = (count * mean + w * prior_mean) / (count + w)\n",
    "\n",
    "        # Replace each value by the according smoothed means\n",
    "        X_loc[posterior] = X_loc[level].map(smooth)\n",
    "        X_loc_test[posterior] = X_loc_test[level].map(smooth)\n",
    "\n",
    "        # Replacing NA's in test data with prior\n",
    "        if i>1:\n",
    "            X_loc_test[posterior].fillna(X_loc_test[prior], inplace=True)\n",
    "\n",
    "X_loc[['y', 'level_3_y_2', 'level_3_y_3']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_feats = ['post_2', 'post_3']\n",
    "\n",
    "X_train['post_2'] = X_loc['level_3_y_2']\n",
    "X_train['post_3'] = X_loc['level_3_y_3']\n",
    "\n",
    "X_test['post_2'] = X_loc_test['level_3_y_2']\n",
    "X_test['post_3'] = X_loc_test['level_3_y_3']\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Preprocessing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General preprocessing pipeline\n",
    "gen_pp = ColumnTransformer(\n",
    "    [('log-transform', FunctionTransformer(np.log1p), ['age', 'area_percentage']), \n",
    "     ('categorical', OneHotEncoder(), cat_feats) \n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating logistic regression preprocessing pipeline\n",
    "logreg_pp = ColumnTransformer(\n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),\n",
    "     ('categorical', OneHotEncoder(), cat_feats),   \n",
    "     ('location', OneHotEncoder(categories=loc_cats), loc_feats),\n",
    "     ('posterior', 'drop', post_feats)\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Creating Logistic Regression pipeline --- hyperparameter has been optimized\n",
    "logreg_pipe = Pipeline([('specific', logreg_pp),\n",
    "                        ('model', LogisticRegression(multi_class='multinomial',\n",
    "                                                     max_iter=1600,\n",
    "                                                     random_state=SEED))])\n",
    "\n",
    "set_config(display='diagram')\n",
    "logreg_pipe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SVM preprocessing pipeline\n",
    "svm_pp = ColumnTransformer(\n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', OneHotEncoder(), cat_feats), \n",
    "     ('location', OneHotEncoder(categories=loc_cats), loc_feats),\n",
    "     ('posterior', 'drop', post_feats)\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "svm_model = OneVsOneClassifier(LinearSVC(random_state=SEED, C=6.5, dual=False))\n",
    "\n",
    "svm_params = {'model__estimator__C': np.logspace(-1,1,20) }\n",
    "\n",
    "# Creating SVM pipeline --- hyperparameter has been optimized\n",
    "svm_pipe = Pipeline([('processing', svm_pp),\n",
    "                     ('model', svm_model)])\n",
    "\n",
    "set_config(display='diagram')\n",
    "svm_pipe  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Random Forest preprocessing pipeline\n",
    "rf_pp = ColumnTransformer( \n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', OneHotEncoder(), cat_feats), \n",
    "     ('location', 'drop', loc_feats),\n",
    "     ('posterior', 'passthrough', post_feats) \n",
    "    ], \n",
    "    remainder='passthrough') \n",
    "\n",
    "# Hyperparameters for Random Forest regressor\n",
    "rf_params = {'model__n_estimators': np.arange(200, 350, 10),\n",
    "             'model__max_depth': np.arange(20, 60, 2),\n",
    "             'model__min_samples_split': np.arange(36, 160, 2),\n",
    "             'model__min_samples_leaf': np.arange(1, 14, 1) \n",
    "             }\n",
    "\n",
    "# Creating Random Forest pipeline  \n",
    "rf_pipe = Pipeline([('specific', rf_pp), \n",
    "                    ('model', RandomForestClassifier(n_estimators=350,\n",
    "                                                     max_depth=40, min_samples_split=46,\n",
    "                                                     min_samples_leaf=8, random_state=SEED))])\n",
    "                    \n",
    "set_config(display='diagram')\n",
    "rf_pipe              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating MLP preprocessing pipeline\n",
    "mlp_pp = ColumnTransformer( \n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', OneHotEncoder(drop='first'), cat_feats), \n",
    "     ('location', 'drop', loc_feats),\n",
    "     ('posterior', 'passthrough', post_feats)\n",
    "    ], \n",
    "    remainder='passthrough') \n",
    "\n",
    "# Creating MLP pipeline\n",
    "mlp_pipe = Pipeline([('preprocessing', mlp_pp),\n",
    "                     ('model', MLPClassifier())])\n",
    "\n",
    "set_config(display='diagram')\n",
    "mlp_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LightGBM preprocessing pipeline\n",
    "catboost_pp = ColumnTransformer(\n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', 'passthrough', cat_feats), \n",
    "     ('location', 'passthrough', loc_feats),\n",
    "     ('posterior', 'drop', post_feats) \n",
    "    ], \n",
    "    remainder='passthrough') \n",
    "\n",
    "# Creating CatBoost pipeline\n",
    "catboost_pipe = Pipeline([('model', CatBoostClassifier(cat_features=cat_feats + loc_feats, \n",
    "                                                       loss_function='MultiClass'))])\n",
    "set_config(display='diagram')\n",
    "catboost_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating XGBoost preprocessing pipeline\n",
    "xgboost_pp = ColumnTransformer(\n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', OneHotEncoder(), cat_feats), \n",
    "     ('location', 'drop', loc_feats),\n",
    "     ('posterior', 'passthrough', post_feats)\n",
    "    ], \n",
    "    remainder='passthrough') \n",
    "# Creating XGBoost pipeline\n",
    "xgboost_pipe = Pipeline([('preprocessing', xgboost_pp), \n",
    "                         ('model', XGBClassifier(objective='multi:softmax', \n",
    "                                                     num_class=3))])\n",
    "\n",
    "set_config(display='diagram')\n",
    "xgboost_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LightGBM preprocessing pipeline\n",
    "lgbm_pp = ColumnTransformer(\n",
    "    [('continuous', make_pipeline(RobustScaler(), PCA(whiten=True)), cont_feats),  \n",
    "     ('categorical', OneHotEncoder(), cat_feats), \n",
    "     ('location', 'drop', loc_feats),\n",
    "     ('posterior', 'passthrough', post_feats)\n",
    "    ], \n",
    "    remainder='passthrough') \n",
    "    \n",
    "# Creating LightGBM pipeline\n",
    "lgbm_pipe = Pipeline([('preprocessing', lgbm_pp),\n",
    "                          ('model', LGBMClassifier(objective='multiclass', \n",
    "                                                   num_class=3))])\n",
    "\n",
    "set_config(display='diagram')\n",
    "lgbm_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running initial 10 - fold cross validation \n",
    "# note: CV does not work with catboost pipe\n",
    "print('Running Cross-Validation ...')\n",
    "#%time cv = cross_val_score(catboost_pipe, X_train.drop(columns=post_feats), y_train, scoring=make_scorer(model_score), cv=5, n_jobs=-1)\n",
    "    \n",
    "print('Finished running')\n",
    "#print('Score: ', cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing best estimators to pass into stacked model\n",
    "final_estimators = [('CatBoost', catboost_pipe),\n",
    "                    ('LightGBM', lgbm_pipe),\n",
    "                    ('XGBoost', xgboost_pipe),\n",
    "                    ('MLP', mlp_pipe), \n",
    "                    ('Random Forest', rf_pipe), \n",
    "                    ('SVM', svm_pipe), \n",
    "                    ('Logistic Regression', logreg_pipe)]\n",
    "\n",
    "methods = {'CatBoost':'predict',\n",
    "           'LightGBM':'predict',\n",
    "           'XGBoost':'predict',\n",
    "           'Random Forest':'predict', \n",
    "           'SVM':'decision_function', \n",
    "           'Logistic Regression':'predict_proba'}\n",
    "#stack = StackingClassifier(estimators=final_estimators, cv=5)\n",
    "\n",
    "#print('Running stacked estimator model...')\n",
    "#%time cv = cross_val_score(stack, X_train, y_train, scoring=make_scorer(model_score), cv=5, n_jobs=-1)\n",
    "#print('Finished running')\n",
    "#print('Score: ', cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator with optimal hyperparameters\n",
    "final_model = StackingClassifier(final_estimator = LogisticRegression(multi_class='multinomial',\n",
    "                                                                      max_iter=1600,\n",
    "                                                                      random_state=SEED), \n",
    "                                 estimators=final_estimators, \n",
    "                                 n_jobs=1)\n",
    "\n",
    "set_config(display='diagram')\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with complete training sample\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Creating submission file\n",
    "preds = final_model.predict(X_test)\n",
    "preds = preds.flatten()\n",
    "output = pd.DataFrame( {'building_id': ID, 'damage_grade': preds})\n",
    "output = output.astype(int)\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
